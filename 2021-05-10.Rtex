%\def\option{}
\providecommand{\option}{handout}

\documentclass[aspectratio=169,notheorems,9pt,\option]{beamer}


%% begin.rcode setup, include=FALSE
% library(knitr)
% opts_chunk$set(fig.path='output/', cache.path='output/cache/latex-')
% require(ggplot2)
% options(width = 110)
% opts_chunk$set(tidy = "styler")
% require(Cairo)
% opts_chunk$set(dev="cairo_pdf")
% opts_chunk$set(cache=TRUE, fig.width=4.5, fig.height=2.5, warning=FALSE)
%% end.rcode

\newcommand{\rinline}[1]{SOMETHING WRONG WITH knitr}
\input{preambulum.tex}

\begin{document}

\maketitle

\begin{frame}{Lineáris regresszió, lineáris modell}
  \begin{itemize}
    \item $(x_i,y_i)$, $i=1,\dots,n$ megfigyelések. 
    Van-e valamilyen függvénykapcsolat $x$ és $y$ között?
    
    Pl. $y$ jövedelem, $x$ életkor. $y$ lakásár, $x$ a lakás jellemzői. 
    \item $y$ neve függő, vagy eredmény változó, $x$ a független vagy magyarázó változó.
    
    Becsüljük az $f\in \F$ függvényt az $(x_i,y_i)$ adatok alapán, ha 
    $y_i=f(x_i)+\eps_i$, ahol $\eps_i$ zaj, 
    azaz független $N(0,\sigma^2)$ változók és $\sigma^2$ 
    ismeretlen.
    
    \item $\F$ általában véges dimenziós vektortér, ha $f_1,\dots,f_p$ bázis $\F$-ben, 
    és $\underbar{x}_i=(f_1(x_i),\dots,f_p(x_i))$, akkor $\F$ tetszőleges eleme $\sum_i \beta_i f_i$ 
    alakú és $\beta$ meghatározása a feladat.
    
    \item Maximum likelihood becslés $\beta$-ra és $\sigma^2$-re
    \begin{displaymath}
      \ell(\beta,\sigma^2)
      =-\frac n2\ln(2\pi\sigma^2)-\frac1{2\sigma^2}\sum_{i=1}^n (y_i-\underbar{x}_i\cdot \beta)^2  
    \end{displaymath}
    Vektor alakban $y$ az $y_i$ értékekből alkotott (oszlop) vektor. Az $X$ mátrix $i$. sora $\underbar{x}_i$.
    \begin{displaymath}
      \ell(\beta,\sigma^2) = -\frac n2\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2}\norm{y-X\beta}^2
    \end{displaymath}
    \item $\ell(\beta,\sigma^2)$ maximális, ha $\norm{y-X\beta}^2$ minimális és $\sigma^2=\frac1n\norm{y-X\hat\beta}^2$ 
    \item $\beta$ maximum likelihood becslése a négyzetes veszteség $\norm{y-X\beta}^2$ minimalizálásával kapható.
    Legkisebb négyzetes becslésnek is hívják (LNB)
    
  \end{itemize}
\end{frame}

\begin{frame}{Példa lineáris regresszió}
  \begin{itemize}
    \item A magyarázó változó lineáris (affin) függvényei között keressük a függvénykapcsolatot: 
    $f(x)=\beta_0+x\beta_1$.
    \begin{displaymath}
      \norm{y-X\beta}^2
      =\sum_{i=1}^n (y_i-\beta_0-\beta_1 x_i)^2
      =n(\bar{y}-\beta_1\bar{x}-\beta_0)^2+ \sum (y_i-\bar{y}-\beta_1 (x_i-\bar{x}))^2
    \end{displaymath}
    \item $\hat\beta_1=\frac{\frac1n\sum_i (y_i-\bar{y})(x_i-\bar{x})}{\frac1n\sum_i (x_i-\bar{x})^2}$, 
    $\hat\beta_0=\bar{y}-\hat\beta_1\bar{x}$. 
    \item Geometriai kép: $y$ és $x$-ből levonjuk az átlagot, feltehető, hogy $\bar{x}=0$, $\bar{y}=0$. 
    Az $y\in\real^n$ vektort vetítjük az $x\in\real^n$ által kifeszített egy dimenziós altérre.
    A vetület $\frac{\ip{x}{y}}{\norm{x}^2} x$ és 
    $\hat\beta_1=\frac{\ip{x}{y}}{\norm{x}^2}$.
    \item $\hat\sigma^2=\frac1n\norm{y-\hat\beta_0-\hat\beta_1x}^2$ nem torzítatlan $\sigma^2$-re. 
    
    Itt $\hat\beta_0+\hat\beta_1 x$ az $y\in\real^n$ vektor vetülete az 
    $x$ és a konstans 1 vektor által 
    kifeszített kétdimenziós altérre. $P$  a merőleges vetítés erre az altérre:
    \begin{displaymath}
      Py=P(\beta_0+\beta_1 x+\eps)=\beta_0+\beta_1+P\eps,\quad
      \norm{y-\hat\beta_0-\hat\beta_1x}^2=\norm{(I-P)\eps}^2,\quad 
      \norm{y-\hat\beta_0-\hat\beta_1x}^2\sim\sigma^2\chi^2_{n-2}
    \end{displaymath}
    $\frac1{n-2}\norm{y-\hat\beta_0-\hat\beta_1x}^2$ torzítatlan $\sigma^2$-re.
  \end{itemize}
\end{frame}



\begin{frame}{Lineáris modell}
  \begin{itemize}
    \item Általánosabban. $X$ $n\times p$ mátrix, a mátrix $i$. sora $x_i$: a magyarázó változók 
    értéke az $i$. megfigyelésben.
    \begin{displaymath}
      y_i=x_i\cdot\beta+\eps_i,\quad\text{ahol $\eps_i\sim N(0,\sigma^2)$ függetlenek}
    \end{displaymath}
    Vektor alakban $y=X\beta+\eps$.
    
    $X$ első oszlopa lehet 1, akkor $\beta_1$ a konstans hatás (intercept).
    
    \item $\beta$ maximum likelihood becslése: 
    $\hat\beta=\arg\min_\beta\norm{y-X\beta}^2$ és $\sigma^2$ maximum likelihood becslése 
    $\hat\sigma^2=\frac1n\norm{y-X\hat\beta}^2$
    \item $\hat y=X\hat\beta$ az $y$ vetülete $X$ képterére. $\im X\perp \ker X^T$, azaz $X^T(y-X\hat\beta)=0$, azaz 
    $X^Ty=(X^TX)\hat\beta$ és $\beta= (X^TX)^{-1}X^Ty$, ha az inverz létezik, azaz $X$ rangja $p$.
    
    
    Ha $(X^TX)$ nem invertálható, akkor $\hat\beta$ nem egyértelmű, de $\hat y=X\hat\beta$  igen. Ilyenkor
    a legkisebb normájú $\hat\beta$-t szokás venni, ami $(X^TX)$ Moore-Penrose inverzével kapható.
    
    \item $\hat\beta$ a likelihood egyenlet megoldásából is megkapható.
    A likelihood egyenlet $\beta$-ra:
    \begin{displaymath}
      \ell(\beta,\sigma^2)=-\frac{n}{2}\ln(2\pi\sigma^2)-\frac1{2\sigma^2}\sum_{i=1}^n(y_i-x_i\cdot \beta)^2,
      \quad
      \partial_{\beta} -\tfrac12\norm{y-X\beta}^2 = \sum_{i=1}^n (y_i-x_i\beta)x_i = (y-X\beta)^T X =0  
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}{$\hat\beta$, $\hat\sigma^2$ eloszlása}
  \begin{itemize}
    \item $n$ megfigyelés, $p$ magyarázó változó. 
    $X$ $n\times p$ méretű mátrix, a sorai a magyarázó változókból álló vektorok.
    \begin{displaymath}
      y=X\beta+\eps,\quad \eps\sim N(0,\sigma^2\Id_n)
    \end{displaymath}
    Ha $X^TX$ invertálható, azaz $X$ rangja $p$, akkor
    $\hat\beta = (X^TX)^{-1}X^T y$.
    \item $\hat\beta$ normális eloszlású és
    \begin{align*}
      \E{\hat\beta}&= (X^TX)^{-1}X^T\E{X\beta+\eps}=\beta,\\
      \Sigma(\hat\beta) &=\Sigma((X^TX)^{-1}X^T y)=(X^TX)^{-1}X^T\Sigma(y) X(X^TX)^{-1}=\sigma^2(X^TX)^{-1}.
    \end{align*} 
    \item $\sigma^2$ becslése a \textbf{rezidulis négyzet összegen} (RSS) 
    $\norm{y-X\hat\beta}^2$ alapul, ahol $X\hat\beta=\hat y$ az $y$ vetülete $X$ képterére.
    
    $y-\hat y$ nem más mint  $y$  vetülete $X$ képterének ortokomplementerére. $y=X\beta+\eps$ miatt $y-\hat y$ az 
    $\eps$ zaj vektor merőleges vetülete $X$ képterének ortokomplementerére. 
    $\norm{y-\hat y}^2\sim \sigma^2\chi^2_{\dim (\im X)^\perp}=\sigma^2\chi^2_{n-p}$.
    
    $\hat\sigma^2=\frac1{n-p}\norm{y-X\hat\beta}^2$ torzítatlan becslés $\sigma^2$-re.
    \item $\hat\beta$ és $\hat\sigma^2$ függetlenek, mert $\hat\beta$-t $y$ $\im X$-re vett vetületéből, 
    míg $\hat\sigma^2$-et $y$ $\im X^{\perp}$-ra vett vetületéből számoljuk.
  \end{itemize}
\end{frame}


%% begin.rcode echo=FALSE, warning=FALSE
% n <- 30
% sigma <- 0.25
% max_x <- 5
% x <- runif(n, 0, max_x)
% df <- data.frame(x=x, y=sin(x)+rnorm(length(x), 0, sigma))
% mk_formula <- function(degree){
%   if(degree==0){
%     return(y~1)
%   }
%   if(degree==1){
%     return(y~x)
%   }
%   txt = paste(sprintf("I((x/5)^(%d))",2:degree), collapse="+")
%   txt = paste("y~x",txt,sep="+")
%   as.formula(txt)
% }
% rss <- function(obj) sum(obj$residuals^2)
% results <- lapply(c(0, 1, 2, 3, 5, 10, 20), 
%   function(d){
%     f <- mk_formula(d)
%     model <- lm(f, data=df)
%     list(model=model, degree=d, rss=rss(model))
% })
% plt <- qplot(x, y, data=df) + coord_equal() 
% plt <- plt + geom_function(fun=sin, color='blue', alpha=0.5)
% pred_fn <- function(obj, x) 
%   if(obj$degree>0){
%     predict(obj$model, list(x=x))
%   }  else {
%     rep(obj$model$coefficients,length(x))
%   }
% plot_result <- function(obj, color='red'){
%   plt + geom_function(fun=function(x) pred_fn(obj, x),  color=color)
% }
%% end.rcode

\begin{frame}{Példa}
  
  \begin{columns}[t]<*>
    \begin{column}[T]{0.5\textwidth}<*>
      \ifdefempty{\option}
      {
      \begin{overprint}
        \only<1>{
          %% begin.rcode  echo=FALSE
          % plt
          %% end.rcode
        }    
        \only<2>{
        %\arabic{beamerpauses}
        %% begin.rcode  echo=FALSE
        % plot_result(results[[1]])
        %% end.rcode
        }  
        \only<3>{
        %% begin.rcode  echo=FALSE
        % plot_result(results[[2]])
        %% end.rcode
        }  
        \only<4>{
        %% begin.rcode  echo=FALSE
        % plot_result(results[[3]])
        %% end.rcode
        }  
        \only<5>{
        %% begin.rcode  echo=FALSE
        % plot_result(results[[4]])
        %% end.rcode
        }  
        \only<6>{
        %% begin.rcode  echo=FALSE
        % plot_result(results[[5]])
        %% end.rcode
        }  
        \only<7>{
        %% begin.rcode  echo=FALSE
        % plot_result(results[[6]])
        %% end.rcode
        }  
        \only<8>{
        %% begin.rcode  echo=FALSE
        % plot_result(results[[7]])
        %% end.rcode
        }       
      \end{overprint}
      }{
        %% begin.rcode  echo=FALSE, cache=FALSE, fig.height=3.5
        % t <- seq(min(x),max(x),length.out = 501)
        % preds<-lapply(results, pred_fn, x=t)
        % names(preds) <- sapply(results, "[[", "degree")
        % preds.df <- data.frame(preds, check.names = FALSE)
        % preds.df$x <- t
        % preds.df.long <- tidyr::pivot_longer(preds.df, 
        %                                      grep("^[0-9]*$",names(preds.df), value=TRUE),
        %                                      names_to = "fokszám",
        %                                      values_to = "yhat")
        % preds.df.long$fokszám <- as.factor(as.integer(preds.df.long$fokszám))
        % plt + geom_line(data=preds.df.long, aes(x=x, y=yhat, color=`fokszám`)) + ylim(-2.5, 2.5) 
        %% end.rcode
      }
      %\value{beamerpauses}
      \begin{align*}
        y_i&=\sin(x_i)+\eps_i,
        \quad \eps_i\sim N(0,\rinline{sigma}^2)\\
        x_i&\sim U(0,\rinline{max_x}),
        \quad i=1,\dots,\rinline{n}
      \end{align*}
    \end{column}
    %\arabic{beamerpauses}
    \begin{column}{0.5\textwidth}
      %\value{beamerpauses}
      %\arabic{beamerpauses}
      \begin{itemize}
        %\setcounter{beamerpauses}{1}
        \item  Null modell: $y_i=\beta_0 + \eps_i$. 
        $\hat\beta_0=\bar{y}=\rinline{format(mean(df$y), digits=4)}$
        
        Reziduális négyzetösszeg: $\rinline{format(results[[1]]$rss, digits=4)}$
        
        \item Lineáris regresszió: $y_i=\beta_0 +\beta_1 x_i+ \eps_i$.
        
        Reziduális négyzetösszeg: $\rinline{format(results[[2]]$rss, digits=4)}$
        
        \item Polinomiális regresszió ($d=\rinline{results[[3]]$degree}$): 
        $y_i=\sum_{r=0}^d\beta_r x_i^r+ \eps_i$.
        %$y_i=\beta_0 +\beta_1 x_i+\beta_2 x^2_i+ \eps_i$.
        
        Reziduális négyzetösszeg: $\rinline{format(results[[3]]$rss, digits=4)}$
        
        \item Polinomiális regresszió ($d=\rinline{results[[4]]$degree}$): 
        %$y_i=\sum_{r=0}^d\beta_r (x_i)^r+ \eps_i$.
        
        Reziduális négyzetösszeg: $\rinline{format(results[[4]]$rss, digits=4)}$
        
        \item Polinomiális regresszió ($d=\rinline{results[[5]]$degree}$): 
        
        
        Reziduális négyzetösszeg: $\rinline{format(results[[5]]$rss, digits=4)}$
        \item Polinomiális regresszió ($d=\rinline{results[[6]]$degree}$): 
        %$y_i=\sum_{r=0}^d\beta_r (x_i)^r+ \eps_i$.
        
        Reziduális négyzetösszeg: $\rinline{format(results[[6]]$rss, digits=4)}$
        
        \item Polinomiális regresszió ($d=\rinline{results[[7]]$degree}$): 
        %$y_i=\sum_{r=0}^d\beta_r (x_i)^r+ \eps_i$.
        
        Reziduális négyzetösszeg: $\rinline{format(results[[7]]$rss, digits=4)}$
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Hipotézis vizsgálat normális lineáris modellben}
  \begin{itemize}
    \item
    \begin{displaymath}
      y\sim N (X\beta,\sigma^2\Id_n),\quad \beta\in\real^p,\,\sigma >0
    \end{displaymath}
    \item Kérdés: Van-e fölösleges magyarázó változó, azaz olyan $j$, hogy 
    $\beta_j=0$, vagy
    általánosabb igaz-e, hogy $C\beta=0$.
    \item $H_0: C\beta=0$, $H_1:C\beta\neq0$.
    \item $\real^n=\cL^R\oplus\cL^1\oplus\cL^0$ három merőleges komponensre bomlik,
    ahol $\cL^R=\im X^\perp$, $\cL^0=\set{X\beta}{C\beta=0}$ 
    és $\cL^1=\im X\cap (\cL^0)^\perp$.
    \item $H_0$ fenállása esetén $X\beta\in\cL^0$ és $y$ vetülete 
    $\cL^R\oplus\cL^1$-re azonos $\eps=y-X\beta\sim N(0,\sigma^2\Id_n)$ vetületével.
    \item Azaz $H_0$ mellett $y=y^R+y^1+X\hat\beta_0$, ahol 
    $\hat\beta_0=\arg\min_{\beta: C\beta=0}\norm{y-X\beta}^2$
    és $y^1\in\cL^1$, $y^R\in\cL^R$.
    \item $\cL^1$, $\cL^R$ merőlegesek, ezért $y^R$ és $y^1$ függetlenek és 
    $\norm{y^R}^2\sim\sigma^2\chi^2_{\dim\cL^R}=\sigma^2\chi^2_{n-p}$, 
    $\norm{y^1}^2\sim\sigma^2\chi^2_{\dim\cL^1}=\sigma^2\chi^2_q$.
    
    Ha $H_0$ nem teljesül, akkor $\norm{y^1}^2= \norm{P_{\cL^1}(X\beta+\eps)}^2$ 
    nem centrális $\chi^2$ eloszlású. $H_1$ mellett $\norm{y^1}^2$ nagy(obb) értékre vezet.

    \item $H_0$ mellett 
    $T=\frac{\frac{1}{q}\norm{y^1}^2}{\frac{1}{n-p}\norm{y^R}^2}\sim F_{q,n-p}$ eloszlású, 
    míg $H_1$ mellett $T$ nagy értékeket ad. $F$ próbát végeztünk egyoldali ellenhipotézis ellen.

  \end{itemize}
\end{frame}

\begin{frame}{Példa}
  \begin{align*}
    y_i&=\sin(x_i)+\eps_i,
    \quad \eps_i\sim N(0,\rinline{sigma}^2)\\
    x_i&\sim U(0,\rinline{max_x}),
    \quad i=1,\dots,\rinline{n}
  \end{align*}

  %% begin.rcode  echo=FALSE, cache=FALSE
  % res.df <- data.frame(
  %   "fokszám" = as.integer(sapply(results, "[[", "degree")),
  %   "dim im X" = as.integer(sapply(results, function(obj) obj$model$rank)),
  %   "RSS" = sapply(results, "[[", "rss"),
  %   check.names = FALSE
  % )
  % res.df$"hatsigma^2" <- res.df$RSS/(n-res.df$`dim im X`)
  % res.df$"F-hányados" <- c(NA, diff(res.df$RSS)/diff(n-res.df$`dim im X`))/res.df$"hatsigma^2"
  % res.df$"p-érték" <- pf(res.df$"F-hányados", 
  %                        c(NA,-diff(n-res.df$`dim im X`)),
  %                        n-res.df$`dim im X`,
  %                        lower.tail=FALSE)
  % res.df$"fokszám" <- as.character(res.df$"fokszám")
  % for(n in c("RSS", "hatsigma^2", "F-hányados", "p-érték")){
  %   res.df[[n]] <- as.character(round(res.df[[n]], 3))
  % }
  %% end.rcode

  %% begin.rcode output='latex', echo=FALSE, cache=FALSE
  % knitr::kable(t(res.df), format='latex', digits=3)
  %% end.rcode

  \bigskip

  \begin{itemize}
    \item Itt egymásba ágyazott modelleket hasonlítunk össze. 
    Az \texttt{F-hányados} statisztikák függetlenek.
    
    \item $\dim\im X$ elméletben a polinom forszáma plusz egy. A számolt 
    értékek ennél kisebbek lehetnek. 
    
    Ok: a becslés QR faktorizációval történik, ami az 
    $X$ mátrix oszlopait ortogonalizálja. Ha egy oszlop ,,majdnem'' 
    a korábbiak által kifeszített altérben van, akkor azt az oszlopot 
    és az együtthatóját kihagyjuk a becslésből.
    
  \end{itemize}
\end{frame}

\begin{frame}{ANOVA táblázat \texttt{R}-ben}
  \texttt{R}-ben részletesebb táblázat is kapható:
  %% begin.rcode eval=FALSE
  % anova(model)
  %% end.rcode
  %% begin.rcode echo=FALSE
  % #, results='asis', comment=''
  % knitr::kable(xtable::xtable(anova(results[[6]]$model)), format='latex', digits=3)
  % #print(xtable::xtable(anova(results[[6]]$model)),
  % #    sanitize.rownames.function=function(x) 
  % #        gsub('I\\(x\\^\\((.+)\\)\\)','I($x^{\\1}$)',x))
  %% end.rcode
  
  Itt az egyes magyározó változók hatása, ortogonalizálva van. 
  $\text{rss}_i=\min\set{\norm{y-X\beta}}{\beta_{j}=0,\,j>i}$, 
  akkor az $i$. magyarázó változó hatása $\text{rss}_{i-1}-\text{rss}_{i}$.
  
  A szórásnégyzet becslése $\frac{1}{n-p}\norm{y-X\hat\beta}^2$ 
  
\end{frame}

\begin{frame}{Wald próba}
  \begin{displaymath}
    y\sim N (X\beta,\sigma^2\Id_n),\quad \beta\in\real^p,\,\sigma >0
  \end{displaymath}
  \begin{itemize}
    \item $\hat\beta=(X^TX)^{-1}X^T y$. Láttuk, hogy $\E{\hat\beta}=\beta$ 
    és $\Sigma(\hat\beta)=(X^TX)^{-1}\sigma^2$.
    \item $\hat\sigma^2=\frac{1}{n-p}\norm{y-X\hat\beta}^2$ független $\hat\beta$-tól.
    \begin{displaymath}
      \frac{\hat\beta_j-\beta_j}{\sqrt{((X^TX)^{-1})_{j,j}\hat\sigma^2}}\sim t_{n-p}
    \end{displaymath}
    \item A $H_0:\beta_j=0$, $H_1:\beta_j\neq 0$ ellenében a 
    fenti próba statisztikával 
    is vizsgálható.
    %% begin.rcode echo=FALSE
    % #, results='asis'
    % knitr::kable(xtable::xtable(results[[5]]$model), format='latex', digits=3)    
    % #print(xtable::xtable(results[[5]]$model),
    % #    sanitize.rownames.function=function(x) 
    % #        gsub('I\\(x\\^\\((.+)\\)\\)','I($x^{\\1}$)',x))
    %% end.rcode
  \end{itemize}
  
\end{frame}

\begin{frame}{Lineáris becslés, becsülhető függvény}
  \begin{displaymath}
    y_i =x_i\cdot \beta+ \eps_i, \quad\eps_i\sim N(0,\sigma^2)
  \end{displaymath}
  A paramétereket az $\set{(y_i,x_i)}{i=1,\dots,n}$ adatok alapján becsüljük.
  Gyakran a cél $y^*$ becslése, ha a magyarázó változó értéke $x^*$ adott.

  \begin{df}
    $T$ lineáris becslés, ha $T (y)$ az $y$ lineáris függvénye,
    létezik $B\in\real^{q\times n}$, $T (y)=By$.
    
    $\psi$  a paraméter becsülhető függvénye, ha létezik rá
    torzítatlan lineáris becslés.
  \end{df}

  \begin{itemize}
    \item Ha $\psi$ becsülhető, akkor $\psi (\beta)=\E{T}=BX\beta=C\beta$.
    \item Ha $X$ rangja $p$, akkor $\beta$ minden lineáris függvénye
    becsülhető.
    
    Ugyanis $\hat \beta= (X^TX)^{-1}X^Ty$ torzítatlan becslés
    $\beta$-ra és ha $\psi (\beta)=C\beta$, akkor $C\hat \beta$  torzítatlan
    lineáris becslés $\psi$-re.
    
    
    \item $\psi$ becsülhető $\iff$ $\psi (\beta)=C\beta=BX\beta$ alakú.
    
    Ugyanis, $\E{X\hat \beta}=P_X\E{y}=P_X\E{\eps+X\beta}=P_XX\beta=X\beta$, azaz
    $X\hat \beta$ torzítatlan $X\beta$-ra, ha $\hat \beta$ legkisebb négyzetes becslés.
  \end{itemize}
\end{frame}

\begin{frame}{Gauss--Markov tétel}
  \begin{theorem}
    Tegyük fel, hogy $\psi (\beta)=C\beta$ becsülhető és
    $\hat \beta$ tetszőleges legkisebb négyzetes becslés $\beta$-ra.
    \begin{itemize}[<*>]
      \item $C\hat \beta$ torzítatlan, lineáris becslés
      $\psi$-re,
      \item $C\hat \beta$ a torzítatlan lineáris becslések között optimális a
      négyzetes veszteségfüggvényre nézve, azaz kovariancia mátrixa a
      legkisebb.
      \item $C\hat \beta$ nem függ az $\hat \beta$ LNB megválasztásától.
    \end{itemize}
  \end{theorem}
  
  \begin{itemize}
    \item $\psi$ becsülhető $\iff$ $\psi (\beta)=C\beta=BX\beta$ alakú. Feltehető,
    hogy $B=BP_X$, azaz $(\im X)^\perp\subset\ker B$.
    \item $\hat \beta$ LNB, $X\hat \beta=P_Xy$. Ebből
    \begin{displaymath}
      \E{X\hat \beta}=P_X\E{y}=P_XX\beta=X\beta,
      \quad\implies\quad
      \E{C\hat \beta}=B\E{X\hat \beta}=BX\beta=C\beta
    \end{displaymath}
    \item Ha $T=Dy$ torzítatlan lineáris becslés $\psi$-re, akkor
    $\E{Dy}=D\E{y}=DX\beta$, másrészt a torzítatlanság miatt $DX\beta=BX\beta$
    minden $\beta$-ra, vagyis $B=DP_X$. Innen %\im X\subset \ker(D-B)$.
    \begin{displaymath}
      \Sigma (Dy)=D\Sigma (y)D^T=\sigma^2DD^T=\sigma^2D (P_X+
      (\Id_n-P_X))D^T\geq
      \sigma^2BB^T= %+\sigma^2D (\Id_n-P_X)D^T\geq
      B\Sigma(P_Xy)B^T=\Sigma (C\hat \beta)
    \end{displaymath}
    \item Ha $\hat \beta$ és $\hat b$ LNB $\beta$-ra, akkor $\frac12 (\hat
    \beta+\hat b)$ is az $\Sigma (C\hat
    \beta)=\Sigma (C\hat b)=\Sigma (C\frac12 (\hat \beta+\hat b))$. Ugyanakkor
    \begin{displaymath}
      \Sigma (C\tfrac12(\hat \beta+\hat b))+\Sigma (C\tfrac12(\hat \beta-\hat b))
      =\tfrac12(\Sigma(C\hat \beta)+\Sigma (C\hat b))=\Sigma(C\hat \beta)
      \quad\implies\quad
      \Sigma (C (\hat \beta-\hat b))=0\quad\iff\quad C\hat \beta = C\hat b
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}{Kereszt--validáció}

  \begin{itemize}
    \item $(y_i,x_i)$, $i=1,\dots,n$. $X$ az $x_i$-kből számolt magyarázó változókat 
    $\phi(x_i)$, mint sorvektorokat tartalmazó mátrix. 
    Pl. $\phi(x)=(1,x,x^2,\dots,x^{p-1})$. 
    \item $y=X\beta+\eps$ lináris modell. 
    $(y_{*},x_{*})$ új adat, $x_{*}$ ismert, $y_{*}$-ot szeretnénk megbecsülni.
    $\phi(x_{*})$ az $x_*$-ból számolt 
    magyarázó változókból álló sorvektor.
    \item Milyen $\phi$-t válasszunk?
    
    A korábbi példában statisztikai módszerekkel próbáltunk választani.
    
    Válasszuk azt, ami a legjobban teljesít az 
    illesztés során nem látott adatokon, a legkisebb a generalizációs hibája.
    
    \item Generalizációs hiba becsléséhez, az adatokat két részre osztjuk. 
    Az egyiken a modellt illesztjük, a másikon az illesztett modell hibáját számoljuk.
    
    \item Lineáris modellnél könnyen számolható, hogy mi lenne $y_i$ becslése, ha 
    az $i$. adatot nem használnánk $\hat\beta$ meghatározásához. 

    Ok. $X^TX$  egy egyrangú mátrix-szal ($x_i\otimes x_i$) módosul, 
    míg $X^Ty$  $x_iy_i$-vel. Ezek hatása a residuálisra 
    $y_i-\hat y_{(i)}=\frac1{1-x_i(X^TX)^{-1}x_i^T}(y-\hat y_i)$.
  \end{itemize}
\end{frame}

  %% begin.rcode echo=FALSE, error=FALSE, message=FALSE, results='hide', cache=FALSE
  % leave_one_out<-function(model){
  %   X <- model.matrix(model)
  %   QR <- model$qr
  %   pv <- 1:QR$rank
  %   R <- qr.R(QR)[pv, pv]
  %   RX <- try(solve(t(R)) %*% t(X)[QR$pivot[pv], ])
  %   if(length(RX)==1 && class(RX)=='try-error'){ RX<-matrix(NA,1,1)}
  %   model$residuals/(1-colSums(RX^2))
  % }
  % oos <- lapply(results, function(obj) mean(leave_one_out(obj$model)^2))
  % err_data <- structure(dim=c(length(oos),2), 
  %                       dimnames=list(NULL,c('fokszám','átlagos hiba')),
  %                       c(lapply(results, "[[", "degree"),
  %                         lapply(oos, round, 3)))
  %% end.rcode

\begin{frame}{Példa}


  \begin{itemize}
  \item Átlagos négyzetes hiba becslése: $\frac1n\sum_i \abs{y_i-\hat y_{(i)}}^2$, 
  ahol $\hat y_{(i)}=\phi(x_i)\cdot \hat\beta_{(i)}$ és $\hat\beta_{(i)}$ az $i$. 
  adat kihagyásával kapott becslés.
  
  %% begin.rcode echo=FALSE, error=FALSE, message=FALSE
  % knitr::kable(t(err_data), format='latex')
  %% end.rcode
  \item Az általánosítási hiba (becslése) a fokszámmal először csökken, majd nő. 
  \item A hiba kétrészre bontható: $\E{y|x}=\sin(x)$-et kell közelíteni $x$ polinomjaival
    a $[0,5]$ intervallumon. Minél nagyobb a fokszám, annál pontosabb a közelítés.

    A fokszám növelésével a $X$ mátrix képterének a dimenziója is nő ezzel $\hat\beta$ kovariancia mátrixa 
    is és így összegében a $y$ becslésének a szórása is nő. 
    \item Általánosabban:  $y=f(x)+\eps$ és $f$-et az adatok alapján egy véges dimenziós 
    függvénytérből akarjuk közelíteni, 
    pl. polinomok a példában.
    
    $\phi_1,\dots,\phi_p$ egy bázis a függvénytérből és
    az illesztést a $y=\phi(x)\beta+\eps$ összefüggés alapján végezzük. 
    
    $\hat\beta$ a $\beta$ becslése az adatok alapján. 
    
    $x$ egy új adat $y=f(x)+\eps$  (független az adatoktól, amiből $\hat\beta$ számoltuk).
    Ekkor 
    \begin{displaymath}
      \E{(y-\hat y)^2}=\D^2{y}+\D^2{\hat y}+\zjel*{\E{y}-\E{\hat y}}^2 
    \end{displaymath}
  \end{itemize}
\end{frame}

\begin{frame}{Regularizáció, Rigde regresszió és LASSO}
  \begin{itemize}
    \item Ahelyett, hogy a függvényteret változtatnánk, 
    (polinom maximális fokszáma a példában), lehet úgy is egyszerű modellt választani, 
    hogy a bonyolultságot büntetjük. 
    
    \begin{displaymath}
      \hat\beta = 
      \arg\min_\beta \frac1{2}\sum_{i=1}^n (y_i-x_i\cdot \beta)^2 
       + \lambda \psi(\beta),
    \end{displaymath}
    ahol $\lambda>0$ és $\psi$ adott, pl. $\psi=\frac12\norm{\beta}^2$. 
    
    \item Ha $\psi(\beta)=\frac12\norm{\beta}_2^2$, akkor az eljárást \textbf{ridge} regressziónak hívják,
    ha $\psi(\beta)=\norm{\beta}_1$ akkor a neve \textbf{LASSO}. 
    A kettő kombinációja elasztikus háló, (elastic net).
    \item A minimum helyen a $\beta$ szerinti derivált eltűnik. 
    Rigde regresszió esetén
    \begin{displaymath}
      -X^T(y-X\beta)+\lambda\beta=0,
      \quad\implies\quad 
      X^T y=(X^TX+\lambda)\beta
      \quad\implies\quad 
      \hat\beta= (X^TX+\lambda)^{-1}X^T y
    \end{displaymath}
    \item LASSO esetében nincs zárt alakban megoldás, de pl. koordinátánkénti 
    minimalizálással könnyen számolható.
    \item $\lambda$ megválasztása kereszt validációval történhet.
  \end{itemize}
\end{frame}

\end{document}
\begin{frame}{Példa}
  
\end{frame}


\end{document}
