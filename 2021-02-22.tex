%\def\option{}
\providecommand{\option}{handout}
\documentclass[aspectratio=169,notheorems,9pt,\option]{beamer}

\input{preambulum.tex}

\begin{document}

\maketitle

\begin{frame}{Ismétlés. Elégséges és minimális elégséges statisztika}
  \begin{df}
    $S$ elégséges a $\cP=\set{\P[\theta]}{\theta\in\Theta}$ eloszláscsaládra nézve, ha 
    a $\P[\theta]{X\in A\given  S}$ feltételes eloszlásoknak létezik közös változata.

    $S$ minimális elégséges, ha elégséges  és $\sigma(S)^*\subset\cS^*$ 
    tetszőleges $\cS$ elégséges $\sigma$-algebrára.
  \end{df}
  \begin{theorem}[Neyman faktoriációs tétel]
    Dominált családban 
    $S$ pontosan akkor elégséges, ha $f_\theta=(g_\theta\circ S)\cdot h$ alakban írható.
  \end{theorem}
  \begin{theorem}[Egyszerűsített, majdnem mindig igaz változat]
    $\cP$ dominált. $S$ elégséges  statisztika pontosan akkor minimális elégséges, ha
    $x\sim y$ $\iff$ $S(x)=S(y)$,
    ahol $x\sim y$, ha $\Theta_x=\set{\theta}{f_\theta(x)\neq0}=\Theta_y$ és 
    $\theta\mapsto\frac{f_\theta(x)}{f_\theta(y)}$ konstans $\Theta_x=\Theta_y$-on.
  \end{theorem}
\end{frame}

\begin{frame}{Példa, Indikátor minta}
  \begin{itemize}
    \item $\Theta=(0,1)$, $\mfX=\smallset*{0,1}^n$, 
      $f_\theta(x)=\P[\theta]{X=x}=\theta^{s}(1-\theta)^{n-s}$, ahol $s=\sum x_i$.
    \item $S=\sum X_i$ minimális elégségessége másképp.
    \item $\cS$ elégséges $\sigma$-algebra. $\P[\theta]{X\in A\given \cS}$ nem függ $\theta$-tól, 
    $T_A$ jelöli a közös változatot.
    \item $S$ a mintaösszeg elégséges, $\E[\theta]{\I{X\in A}-T_A\given S}=h(S)$ a közös változat. Ekkor 
    \begin{displaymath}
      \E[\theta]{h(S)}=\E[\theta]{\I{X\in A}-T_A}
      =\P[\theta]{A}-\E[\theta]{\E[\theta]{\I[A]\given \cS}}=0.
    \end{displaymath}
    \item $S$ binomiális eloszlású $n$-renddel 
    és $\theta$ paraméterrel:
    \begin{displaymath}
      0=\E[\theta]{h(S)}=
      \sum_{k=0}^n h(k)\binom n k \theta^k(1-\theta)^{n-k}\quad\text{minden $\theta\in(0,1)$}.
    \end{displaymath}
    Így $h(k)=0$, $k=0,1,\dots,n$ és $0=h(S)=\E[\theta]{\I[A]-T_A\given S}$.
    \item Ha $A\in\sigma(S)$, akkor $h(S)=\I[A]-\E{T_A\given S}=0$, azaz $\E{\E{\I[A]\given \cS}\given S}=\I[A]$.
    \item A feltételes várható érték képzés $L^2$-ben merőleges vetítés, 
    ha nem csökkenti az $L^2$ normát akkor nem is változtathat. 
    \item $\I[A]\sim \cS^*$  minden $A\in\sigma(S)$-re, vagyis $\sigma(S)\subset \cS^*$, 
    amiből $\sigma(S)^*\subset\cS^*$ és $\sigma(S)$ minimális elégséges.    
  \end{itemize}
\end{frame}


\begin{frame}{Teljesség}
    \begin{df} 
      $\cP=\set*{\P[\theta]}{\theta\in\Theta}$ eloszláscsalád, 
      $S$ statisztika. $S$ \textbf{teljes statisztika} (a $\cP$ eloszláscsaládra nézve), ha 
      \begin{displaymath}
        \E[\theta](h(S))=0,\quad\text{minden $\theta\in\Theta$-ra}\implies \P[\theta]{h(S)=0}=1,\quad\text{minden $\theta$-ra}
      \end{displaymath} 
      Azaz $S$ teljes, ha az $S$ függvényei között csak a lényegében nulla változóra teljesül, hogy  
      az eloszlás család minden tagja mellett létezik és nulla a várható értéke.
  
      $S$ \textbf{korlátosan teljes}, ha az $S$ \textbf{korlátos} függvényei között csak a 
      lényegében nulla változóra teljesül, hogy  
      az eloszlás család minden tagja mellett  nulla a várható értéke.
    \end{df}
    \continue
    $S$ teljessége az $S$ eloszlásai családjának tulajdonsága.
  
    pl. $\cP$ dominált: $f_\theta=\frac{d\P[\theta]}{d\lambda}$ a % $\lambda$ 
    % domináló mértékre vonatkozó 
    sűrűségek. %$d\tlambda= hd\lambda$ és 
    Ekkor $S:\mfX\to\real^p$ eloszlásainak családját $\mu=\lambda\circ S^{-1}$ dominálja: %abszolút folytonos $\mu$-re nézve:
    \begin{displaymath}
      0=\mu(H)=\lambda(S\in H)\quad\implies\quad (\P[\theta]\circ S^{-1})(H)=\P[\theta]{S\in H}=\int_{S\in H} f_\theta d\lambda=0,\quad\forall \theta. 
      % \P[\theta]{S\in H}=\int_\mfX\I{S(x)\in H}f_\theta\lambda(d x)
      %                   =\int_{\real^p}\I{s\in H}g_\theta(s)\mu(d s).
    \end{displaymath}
    Azaz, $S$ eloszlásainak van sűrűségfüggvénye $\mu$-re nézve % domináló mértékre nézve, 
    % legyen 
    $\tf_\theta=\frac{d(\P[\theta]\circ S^{-1})}{d\mu}$.
    $S$ teljes, ha 
    \begin{displaymath}
      \set{h}{\text{$\int_{\real^p} h\tf_\theta d\mu$ létezik és 0 minden $\theta$-ra}}
      =\set{h}{\text{$h\tf_\theta=0$ a $\mu$ majdnem mindenütt minden $\theta$-ra}}
    %   \E[\theta]{h(S)}=\int_{\real^p}h(s)g_\theta(s)\mu(d s)=0,
    %   \forall \theta\in\Theta
    %   \quad\implies\quad
    %   \P[\theta]{h(S)\neq 0}=\int_{\real^p} \I{h(s)\neq0} g_\theta(s)\mu(d s)=0,\forall \theta\in\Theta.
    \end{displaymath}
  \end{frame}
  

  \begin{frame}{Teljesség}
    \begin{df}
      $\cP=\set*{\P[\theta]}{\theta\in\Theta}$ eloszláscsalád, 
      $S$ statisztika. $S$ \textbf{teljes statisztika} (a $\cP$ eloszláscsaládra nézve), ha 
      \begin{displaymath}
        \E[\theta](h(S))=0,\quad\text{minden $\theta\in\Theta$-ra}\implies \P[\theta]{h=0}=1,\quad\text{minden $\theta$-ra}
      \end{displaymath} 
      Azaz $S$ teljes, ha az $S$ függvényei között csak a lényegében nulla változóra teljesül, hogy  
      az eloszlás család minden tagja mellett létezik és nulla a várható értéke.
  
      $S$ \textbf{korlátosan teljes}, ha az $S$ \textbf{korlátos} függvényei között csak a 
      lényegében nulla változóra teljesül, hogy  
      az eloszlás család minden tagja mellett  nulla a várható értéke.
    \end{df}
    \begin{theorem}
      Ha $S$ korlátosan teljes és elégséges a $\cP$ eloszláscsaládra, akkor minimális elégséges. 
    \end{theorem}
    \begin{itemize}
      \item $A\in\sigma(S)$, $\cS$ tetszőleges elégséges $\sigma$-algebra, $T_A=\P[\theta]{X\in A\given  \cS}$ közös változat.
      \item $h(S)=\E[\theta]{\I[A]-T_A\given  S}=\I[A]-\E{T_A\given  S}$ korlátos statisztika, 
      $\E[\theta]{h(S)}=\P[\theta]{A}-\P[\theta]{A}=0$.
      \item Teljesség miatt $h(S)=0$, azaz $\I[A]=\E{T_A\given  S}$ $\cP$ majdnem mindenütt. $0\leq T_A\leq 1$ miatt
      \begin{displaymath}
        \E[\theta]{(\I[A]-T_A)^2\given  S}=\E{\I[A]-2\I[A]T_A+T_A^2\given  S}
        = \E{T_A^2\given  S}-\I[A]\leq\E{T_A\given  S}-\I[A]=0.
      \end{displaymath}
      \item $\P[\theta]{\I[A]=T_A}=1$ minden $\theta$-ra, 
      azaz $\P[\theta]{A\circ \event*{T_A=1}}=0$, így $A\in\cS^*$ és $\sigma(S)\subset\cS^*$.
    \end{itemize}
  \end{frame}

  
  \begin{frame}{Példa. Nem minden minimális elégséges statisztika korlátosan teljes 
    és nem minden teljes statisztika elégséges}
    \begin{itemize}
      \item $U(\theta,\theta+1)$, $\theta\in\real$ eloszlásból származó, $n\geq2$ elemű minta.
      \item $S=(X_1^*,X_n^*)$ minimális elégséges statisztika.
      \item $X_n^*-X_1^*$ eloszlása nem függ $\theta$-tól és $\E[\theta]{X_n^*-X_1^*}=\frac{n-1}{n+1}$.
      \item $h(S)=(X_n^*-X_1^*)\wedge1-\frac{n-1}{n+1}$ olyan korlátos függvénye $S$-nek, amire
      \begin{displaymath}
        \E[\theta]{h(S)}=0,\quad\forall \theta\in\real.
      \end{displaymath}
      \item A $T=0$ konstans statisztika teljes tetszőleges eloszláscsaládra nézve, de csak a triviális esetben elégséges.
      \item HF. $X$ $n=2$ elemű minta $U(0,\theta)$, $\theta>0$ eloszlásból. 
      Ekkor $T(x)=x_1+x_2$ teljes, de nem elégséges. 
       
    \end{itemize}
  \end{frame}
  
  \begin{frame}{További példa teljes statisztikára}
    \begin{itemize}
      \item $N(\theta,1)$ eloszlásból származó $n$ elemű mintánk van.
      \item $S=\sum X_i\sim N(n\theta,n)$ elégséges statisztika.
      \item 
      \begin{displaymath}
        0=\E[\theta]{h(S)}=\int h(x)\frac{1}{\sqrt{2\pi n}} e^{-\frac{(x-n\theta)^2}{2n}}d x 
        =C(\theta)\int h(x)e^{-\frac{x^2}{2n}}e^{\theta x}d x\quad\forall\theta\in\real  
      \end{displaymath} 
    \end{itemize}
    \begin{theorem}
      $h:\real\to\real$ mérhető, $a<b$, 
      \begin{displaymath}
        \int h(x)e^{\theta x} d x,\quad\text{létezik és nulla minden $\theta\in(a,b)$-re}.
      \end{displaymath} 
      Ekkor $h=0$ Lebesgue majdnem mindenütt.
    \end{theorem}
    \begin{itemize}
      \item A tétel alapján, $h(x)e^{-\frac{x^2}{2n}}=0$ mm. és így $\P[\theta]{h(S)=0}=1$ minden $\theta$-ra
      \item $S$ teljes és elégséges, ezért minimális elégséges.
    \end{itemize}
  \end{frame}
  
  \begin{frame}{Momentum generáló függvény}  
    $X$ $p$-dimenziós vektor változó, $M_X(t)=\E{e^{t\cdot X}}$, $t\in\real^p$ az $X$ momentum generáló függvénye. 
    \begin{proposition}
      Ha $X$ momentum generáló függvénye véges az origó egy
      környezetében, akkor meghatározza $X$ eloszlását.
    \end{proposition}
    \continue
    Bizonyítás vázlat.
    \begin{itemize}   
    \item $X$ skalár változó. $h (z)=h (x+iy)=\E{e^{(x+iy) X}}$  a képzetes tengely körüli
      sávban definiált és véges. Itt deriválható (komplex értelemben),
      ezért a valós tengelyen felvett értékek (a momentum generáló függvény)
      meghatározzák a karakterisztikus függvényt, az pedig az eloszlást.
   
    \item $X$ vektor változó. $\alpha$ rögzített vektor, $Y=\alpha\cdot
      X$ skalár változó. $\E{e^{tY}}=\E{e^{(t\alpha)X}}$, azaz $Y$ eloszlását
      $X$ momentum generáló függvénye meghatározza.
    \item $X$ eloszlását az $\alpha\cdot X$ alakú változók eloszlása
      meghatározza, ugyanis a karakterisztikus függvényekre
      $\phi_{X} (\alpha)=\phi_{\alpha\cdot X} (1)$.
    \item Összefoglalva, a momentum generáló függvény meghatározza az
      egy dimenziós vetületek eloszlását, azok pedig $X$ eloszlását.
    \end{itemize}
    
  \end{frame}
  
  \begin{frame}%%{Elégséges feltétel teljességre}
    \begin{theorem}
      Legyen $\mu$ mérték $\B (\real^p)$-n, $h:\real^p\to\real$ mérhető és 
      \begin{displaymath}\textstyle
        A=\set*{\alpha\in\real^p}{\text{$\int_{\real^p} e^{\alpha \cdot x}h (x)\mu
          (dx)$ létezik és nulla}}%\quad\text{nem üres nyílt}
      \end{displaymath}
      Ha $A$-nak létezik belső pontja, akkor $h$ $\mu$ majdnem mindenütt nulla.
    \end{theorem}
    \begin{itemize}
    \item Legyen $\alpha_0\in\interior A$, és
      \begin{displaymath}
        \nu_{\pm} (H)=\int e^{\alpha_0x}\abs*{h}_\pm (x) \mu (d x),\quad H\in\B(\real^p).
      \end{displaymath}
      $\nu_{+}$, $\nu_{-}$ véges Borel mértékek,  hiszen $\alpha_0\in A$.
    \item  Ha $\nu_{+} (\real^p)=\nu_{-} (\real^p) = 0$, akkor $h$ $\mu$ majdnem mindenütt
      nulla.
    \item Ha $\nu_{+}(\real^p)=\nu_{-} (\real^p)>0$, akkor konstanssal
      való szorzás után feltehető, hogy $\nu_{+},\nu_{-}$ valószínűségi
      mértékek. Azt akarjuk megmutatni, hogy ez az eset nem fordulhat
      elő. 
      
    \item A két mérték momentum generáló függvénye az origó egy
      környezetében azonos, hiszen
      \begin{displaymath}
        \int e^{t x}\nu_{+} (d x)-\int e^{t x}\nu_{-} (d x)=\int
        e^{(t+\alpha_0)x}h (x)\mu (d x)=0
            \quad\text{ha $\alpha_0+t\in A$.}
      \end{displaymath}
    \item Mivel a momentum generáló függvény az eloszlást meghatározza,
      $\nu_{+}=\nu_{-}$ és %$\nu_{+} (h>0)=\nu_{-} (h>0)=0$
      \begin{displaymath}
        \nu_{-} (h>0)=0=\nu_{+}(h>0)=
        \int \I{h (x) >0} e^{\alpha_0 x}\abs*{h}_+ (x)\mu (d x)=
        \nu_{+} (\real^p)=1,
        \quad\text{\alert{\Large\Lightning}}
      \end{displaymath}
      % Ez ellentmondás, abból indultunk ki, hogy feltehető, hogy $\nu_{+} (\real^p)=1$.
    \end{itemize}
  \end{frame}
  
  \begin{frame}{Exponenciális család}
    \begin{df}
      $\cP$ exponenciális eloszlás család, ha dominált és alkalmas paraméterezéssel 
      $\cP=\set*{\P[\theta]}{\theta\in\Theta}$, $\Theta\subset\real^p$ 
      %$\set*{\P[\theta]}{\theta\in\Theta}$ dominált
      %eloszláscsalád, $\Theta\subset\real^p$ nyílt. Ha 
      valamint alkalmas domináló mértékkel, $T:\mfX\to\real^p$ statisztikával 
      a sűrűségfüggvények alakja 
      \begin{displaymath}
        f_{\theta} (x) =\exp{\theta\cdot T(x)-b (\theta)}.\tag{*}
      \end{displaymath}
      %akkor \textbf{exponenciális családról} beszélünk.
    \end{df}
    \continue
    Tulajdonságok:
    \begin{itemize}
    \item $T$ elégséges statisztika (Neyman faktorizációs tétel)
    \item 
    Ha $\Theta\subset\real^p$-ben van $p+1$ általános helyzetű pont, akkor $T$ minimális elégséges.
  
    $\theta_i\in\Theta$, $i=0,\dots,p$ és $\theta_i-\theta_0$ lineáris burka $\real^p$.  
    
    Ekkor, ha $\frac{f_{\theta_i}(x)}{f_{\theta_i}(y)}$ nem függ $\theta$-tól, 
    akkor $(\theta_i-\theta_0)(T(x)-T(y))=0$, 
    $i=1,\dots,p$ és $T(x)=T(y)$, ami a minimális elégségesség feltétele.
  
    Exponenciális család mindig megadható úgy is, hogy $T$ minimális elégséges legyen. HF.
    % lineáris burka $\real^p$, 
    % azaz ha $\Theta\subset\real^p$ tartalmazza $\real^p$ egy (lineáris) bázisát. 
    % Altérre áttérve ez elérhető.
    
    % $T$ mindig választható minimális elégségesnek, elég 
    % $P T$-t minimális elégséges, ahol $P$ merőleges 
    % vetítés  $\Theta$ lineáris burkára.
  
    \item Ha $\Theta$ belseje nem üres, akkor $T$ teljes is.
    
    \end{itemize}
  \end{frame}
  
  \begin{frame}{Exponenciális család teljessége}
    \begin{df}
      $\cP$ exponenciális eloszlás család, ha dominált és alkalmas 
      $\cP=\set*{\P[\theta]}{\theta\in\Theta}$, $\Theta\subset\real^p$ paraméterezéssel 
      %$\set*{\P[\theta]}{\theta\in\Theta}$ dominált
      %eloszláscsalád, $\Theta\subset\real^p$ nyílt. Ha 
      és alkalmas domináló mértékkel, $T:\mfX\to\real^p$ statisztikával 
      a sűrűségfüggvények alakja 
      \begin{displaymath}
        f_{\theta} (x) =\exp{\theta\cdot T(x)-b (\theta)}.\tag{*}
      \end{displaymath}
      %akkor \textbf{exponenciális családról} beszélünk.
    \end{df}
    \continue
    Ha $\Theta$ belseje nem üres, akkor $T$ teljes is.
    \begin{itemize}  
    \item $\lambda$ a domináló mérték, amivel a sűrűség $(*)$ alakú. $\tlambda(H)=\lambda(T\in H)$, $H\in\B(\real^p)$.
  
    \item Ha minden $\theta\in\Theta$-ra, $\E[\theta](h(T))$ létezik és nulla, akkor
    \begin{displaymath}
      0=\E[\theta]{h(T)}=\int_{\mfX} h\circ T \exp{\theta T-b(\theta)}d\lambda 
      =\int_{\real^p} h(t) \exp{\theta t-b(\theta)}\tlambda(d t)
      \implies \int_{\real^p} e^{\theta t} h(t)\tlambda(d t). 
      %,\quad \forall \theta\in\Theta
    \end{displaymath}
  
    \item $h d\tlambda$ az azonosan nulla mérték. Tetszőleges $H\in\B(\real^p)$-re
    \begin{displaymath}
      0=\int_H h d\tlambda=\int_{\mfX} \I{T\in H} h\circ T d\lambda,\quad
      \forall  H\in\B(\real^p)
      \quad\implies\quad \lambda(h\circ T\neq0)=0. 
    \end{displaymath}
    $\P[\theta]{h(T)\neq 0}=0$, azaz $\P[\theta]{h(T)=0}=1$.
    \end{itemize}
  \end{frame}
   
  \begin{frame}{Példák exponenciális családra}
    \begin{itemize}
      \item $n$ elemű minta $\exp(\theta)$, $\theta>0$ eloszlásból, $\mfX=(0,\infty)^n$, $\lambda$ a Lebesgue mérték.
      \begin{displaymath}
        f_\theta(x) = \theta^n\exp{-\theta\sum x_i},\quad T(x)=-\sum x_i,\quad b(\theta)=-n\ln\theta.
      \end{displaymath}
      \item $n$ elemű indikátor minta, $p\in(0,1)$ paraméterrel, $\mfX=\smallset*{0,1}^n$, 
       $\lambda$ a számláló mérték
      \begin{align*}
        f_\theta(x)&=p^{\sum x_i}(1-p)^{n-\sum x_i}=\zfrac*{p}{1-p}^{\sum x_i} (1-p)^n\\
        &=\exp{\theta T(x) - b(\theta)},\quad \theta=\ln\frac{p}{1-p}, \quad p =\frac{1}{1+e^{-\theta}},\quad 
        T(x)=\sum x_i,\quad 
        b(\theta)= n\ln(1+e^{\theta})
      \end{align*}
      \item $N(\mu,\sigma^2)$ eloszlásból származó $n$ elemű minta. $\mfX=\real^n$ $\mu\in\real$, $\sigma^2>0$. 
      Sűrűségfüggvény a Lebesgue mértékre.
      \begin{align*}
        f_{\mu,\sigma^2}(x)&=\frac{1}{(2\pi)^{n/2}\sigma^n}\exp{-\frac1{2\sigma^2}\sum(x_i-\mu)^2}\\
        &=\exp{-\frac{1}{2\sigma^2}\sum x_i^2+\frac{\mu}{\sigma^2}\sum x_i -\tilde b(\mu,\sigma^2)},
        \quad \theta=\zjel*{\tfrac1{2\sigma^2},\tfrac\mu{\sigma^2}}, \quad T(x)=\zjel*{-\sum x_i^2,\sum x_i}.
      \end{align*}
      \item HF. A gyakorlaton szereplő további ,,szép'' eloszláscsaládok: geometriai, Poisson, Gamma, Béta, stb. 
      mind exponenciális családot alkotnak. 

      Kivételek: ha a minimális elégséges statisztika a rendezett minta pl. Cauchy eltolás paraméteres család, 
      ill. amikor az eloszlások tartója nem közös, pl. ,,egyenletes eloszlás''.
    \end{itemize}
  \end{frame}
  
  \begin{frame}{$T$ nem mindig teljes}
    \begin{itemize}
      \item $N(\mu,\sigma^2)$ eloszlásból származó minta $\mu=\sigma^2$  ismeretlen.
      
      Átparaméterezés után $\theta(\mu,\sigma^2)=(\tfrac1{2\sigma^2},\tfrac\mu{\sigma^2})$,
      $\Theta=\set*{(\tau,1)}{\tau>0}$. $T(x)=(-\sum x_i^2,\sum x_i)$ minimális elégséges.
  
      \item $n=2$, 
      \begin{align*}
        \E{T_1}&= \E{-(X_1^2+X_2^2)}=-2(\sigma^2+\mu^2),\\
        \E{T_2}&= \E{(X_1+X_2)} =2\mu,\quad
        \E{T^2_2}=\E{(X_1+X_2)^2} = 2\sigma^2+(2\mu)^2
      \end{align*}
      \item 
      \begin{displaymath}
        \E{2T_1+T_2^2 + T_2} = -4(\sigma^2+\mu^2)+2\sigma^2+4\mu^2+2\mu=2\mu-2\sigma^2=0  
      \end{displaymath}
      $\E[\theta]{h(T)}=0$, minden $\theta=(\tau,1)$-re, ha 
      $h(t_1,t_2)=2t_1+t_2^2+ t_2$.   
      $\E[\theta]{h(T)}=0$ miden $\theta$-ra, de $\P[\theta]{h(T)=0}=0$.
      \begin{displaymath}
        h(T(X))=-(X_1-X_2)^2+2(X_1+X_2)
        %\P[\theta]{-X_1^2+X_1=c}=\P[\theta]{X_1\in\smallset*{x_1,x_2}}=0, 
        %\quad x_{1,2}=\frac{1\pm\sqrt{1-4c}}{2} 
      \end{displaymath}
      Itt $(X_1-X_2,X_1+X_2)$ független nem elfajult normális változók, 
      így $\P[\theta]{h(T)=0}=0$ minden $\theta\in\Theta$-ra. 
      % \begin{displaymath}
      %   \P[\theta]{h(T)=0}=\P[\theta]{-X_1^2+X_1=\sum_{k=2}^n -X_k^2+X_k}=0.
      % \end{displaymath}
  
    \end{itemize}  
  \end{frame}
  
  \begin{frame}{Basu tétele}
    \begin{df}
      $T$ kísérő (ancillary) statisztika (a $\cP$ eloszláscsaládra nézve), ha 
      $\P{T\in A}$ nem függ $\P\in\cP$-től, azaz $T$ eloszlása azonos mindegyik $\P\in\cP$ mellett.
    \end{df}
    \begin{theorem}
      Ha $T$ kísérő statisztika, $S$ korlátosan teljes és elégséges a $\cP$ eloszláscsaládra nézve, 
      akkor $T$ és $S$ független mindegyik $\P\in\cP$ mellett.
    \end{theorem}
    \continue
    $\P[\theta]\in\cP=\set{\P[\theta]}{\theta\in\Theta}$
    \begin{displaymath}
      \P[\theta]{T\in A\given S}-\P[\theta]{T\in A}=h(S)  
    \end{displaymath}
    mert $S$ elégséges és $T$ kísérő.  $\E[\theta]{h(S)}=\P[\theta]{T\in A}-\P[\theta]{T\in A}=0$. 
    Mivel $S$ korlátosan teljes, ezért $\P[\theta]{h(S)=0}=1$ miden $\theta$-ra, azaz $\P[\theta]{T\in A\given S}=\P[\theta]{T\in A}$ és
    \begin{displaymath}
      \P[\theta]{T\in A, S\in B}=\E[\theta]{\P{T\in A\given  S}\I{S\in B}}
      =\P[\theta]{T\in A}\P[\theta]{S\in B}, \quad\text{minden $A,B$-re}
    \end{displaymath}
  \end{frame}

  \begin{frame}{Példák kísérő statisztikákra}
    \begin{itemize}
      \item $X_1,\dot,X_n$ exponenciális eloszlású minta $\lambda>0$ paraméterrel. $T_i=X_i/\sum X_i$, $T=(T_1,\dots,T_n)$. 
      Ekkor $T$ kísérő statisztika, ugyanis $\P[\lambda]$ alatt $X$ ugyanolyan eloszlású, mint $\frac1\lambda Z$, ahol $Z_1,\dots,Z_n$
      független egységnyi paraméterű exponenciálisok és $T$ kiszámításakor $\lambda$ kiesik. $S=\sum X_i$ teljes és elégséges, 
      azaz $T$ és $S$ független.

      Sűrűségtranszformációs formulával is könnyen ellenőrizhető.

      Ugyanígy kísérő $T'$, ha $T'_i=X_i/\norm{X}_2$ és $T'$ és $S$ is független.
      \item Hasonlóan tetszőleges skálaparaméteres családban $X/\norm{X}_p$ kísérő statisztika.
      \item $X_1,\dots,X_n$ $N(\mu,\sigma^2)$ eloszlású minta, ahol $\mu\in\real$, $\sigma>0$. Ekkor $X$ ugyanolyan eloszlású,
       mint $\mu+\sigma Z$, ahol $Z_1,\dots,Z_n$ független standard normálisok. Így 
       \begin{displaymath}
        T(X)=\frac{X_1-\bar{X}}{s_n(X)}\eqinlaw\frac{Z_1-\bar{Z}}{s_n(Z)}
        \quad\text{kísérő statisztika, ahol} \bar{X}=\frac1n\sum_i X_i,\quad\text{és}\quad
        s_n^2(X)=\frac1n\sum_i {X_i-\bar{X}}^2
       \end{displaymath}
       $S=(\sum X_i,\sum X_i^2)$ teljes és elégséges, vagyis független $T$-től.
    \end{itemize}  
  \end{frame}
  
  \begin{frame}{Rao-Backwell-Kolmogorov tétel}
    \begin{df}
      $\cP=\set{\P[\theta]}{\theta\in \Theta}$. $g:\Theta\to\real$. 

      $T$ statisztika a $g(\theta)$ torzítatlan becslése, ha $\E[\theta]{T}=g(\theta)$.

      $T$ a $g(\theta)$ hatásos becslése, ha torzítatlan 
      és tetszőleges $T'$ torzítatlan becslésre $\D[\theta]^2{T}\leq\D[\theta]^2{T'}$.
    \end{df}
    \begin{theorem}
      $S$ elégséges a $\cP$ eloszláscsaládra és $T$ torzítatlan $g:\Theta\to\real$-re. Ekkor $\E[\theta]{T\given  S}=h(S)$ 
      közös változata torzítatlan $g(\theta)$-ra és $\D[\theta]^2{h(S)}\leq \D[\theta]^2{T}$.

      Ha az $S$ statisztika teljes is, akkor $\E[\theta]{T\given  S}$ hatásos becslés.
    \end{theorem}
    \begin{itemize}
      \item $S$ elégséges, ezért $h(S)=\E[\theta]{T\given  S}$ nem függ $\theta$-tól!
      \item $\E[\theta]{h(S)}=\E[\theta]{T}=g(\theta)$ a teljes várható érték tétel miatt, azaz $h(S)$ torzítatlan.
      \item $\D[\theta]^2(h(S))=\E[\theta]{(h(S)-g(\theta))^2}\leq \E[\theta]{(T-g(\theta))^2}=\D[\theta]^2(T)$, mert
      \begin{displaymath}
        (h(S)-g(\theta))^2=\zjel{\E[\theta]{T\given  S}-g(\theta)}^2\leq\E{(T-g(\theta))^2\given  S}
      \end{displaymath}
      a Jensen egyenlőtlenség szerint.
    \end{itemize}
    
  \end{frame}

  \begin{frame}{Rao-Backwell-Kolmogorov tétel}
    \begin{df}
      $\cP=\set{\P[\theta]}{\theta\in \Theta}$. $g:\Theta\to\real$. 

      $T$ statisztika a $g(\theta)$ torzítatlan becslése, ha $\E[\theta]{T}=g(\theta)$.

      $T$ a $g$ hatásos becslése, ha torzítatlan 
      és tetszőleges $T'$ torzítatlan becslésre $\D[\theta]^2{T}\leq\D[\theta]^2{T'}$.
    \end{df}
    \begin{theorem}
      $S$ elégséges a $\cP$ eloszláscsaládra és $T$ torzítatlan $g:\Theta\to\real$-re. Ekkor $\E[\theta]{T\given  S}=h(S)$ 
      közös változata torzítatlan $g(\theta)$-ra és $\D[\theta]^2{h(S)}\leq \D[\theta]^2{T}$.

      Ha az $S$ statisztika teljes is, akkor $\E[\theta]{T\given  S}$ hatásos becslés.
    \end{theorem}
    \begin{itemize}
      \item Ha $S$ teljes és elégséges,  $T,T'$ tetszőleges torzítatlan becslések $g(\theta)$-ra. $h(S)=\E[\theta]{T\given  S}$.
      \item $\E[\theta]{T-T'\given  S}=\th(S)$ és $\E[\theta]{\th(S)}=g(\theta)-g(\theta)=0$ így $\th(S)=0$ és $\E{T'\given  S}=h(S)$.
      \item  $\D^2{h(S)}\leq \D^2{T'}$ tetszőleges $T'$ torzítatlan becslésre, vagyis $\E{T\given  S}$ hatásos.
    \end{itemize}
    \continue
    \begin{itemize}
      \item $W(t,\theta)$ \textbf{veszteségfüggvény}, pl. $W(t,\theta)=\abs{t-g(\theta)}^2$ a négyzetes veszteségfüggvény. 
      $R_T(\theta)=\E[\theta]{W(T,\theta)}$ a $T$ becslés \textbf{rizikó}ja. 
      A torzítatlan becslések körében a négyzetes rizikó a szórásnégyzet. 
      \item $W$ az első változóban konvex, $S$ teljes és elégséges, $T$ torzítatlan, akkor 
      $\E[\theta]{T\given  S}$ közös változata minimális rizikójú a torzítatlan becslések között: $R_T\leq R_{T'}$ tetszőleges 
      torzítatlan $T'$-re. % becslésre.
    \end{itemize}
  \end{frame}

  \begin{frame}{Példa}
    \begin{itemize}
      \item $X_1,\dots,X_n$ $n$ elemű indikátor minta $p\in(0,1)$ paraméterrel. 
      $S=\sum X_i$ teljes és elégséges. $g(p)=p^k$, ahol $1\leq k\leq n$.
      
      $T(X)=\I{X_1=1,\dots,X_k=1}$ torzítatlan becslés $p^k$-ra. $\E{T\given  S}$ hatásos.
      \begin{align*}
        \E{T\given  S=s}&=\frac{\P{T=1,S=s}}{S=s}\\
        &=\frac{\P{T=1,\sum_{i=k+1}^n X_i=s-k}}{\P{S=s}}=
        \begin{cases}
          0&\text{ha $s<k$}\\
          \frac{p^k\binom{n-k}{s-k}p^{s-k}(1-p)^{(n-k)-(s-k)}}{\binom{n}s p^s(1-p)^{n-s}}=\frac{s\cdots(s-(k-1))}{n\cdots(n-(k-1))}&\text{ha $k\leq s\leq n$}
        \end{cases}
      \end{align*}
      $\E{T\given  S}=\frac{S}n\cdot\frac{S-1}{n-1}\cdots\frac{S-(k-1)}{n-(k-1)}$ hatásos becslés $p^k$-ra.
    \end{itemize}  
  \end{frame}

  \begin{frame}{Példa}
    \begin{itemize}
      \item Egy gépről kikerülő termék jellemző mérete, például sugara $R\sim N(\mu,\sigma^2)$ eloszlású, $\mu>0,\sigma^2>0$. 
      \item A termék selejtes, ha $R>c$, ahol $c$ adott konstans.
      \item $n$ elemű $X_1,\dots,X_n$ minta alapján szeretnénk a selejt arányt becsülni.
      \item $S=(\sum X_i,\sum X_i^2)$ teljes elégséges statisztika.
      \item $T=\I{X_1>c}$ torzítatlan becslés a selejt arányra.
      \begin{displaymath}
        \E{T\given  S}=\P{X_1>c\given  S}=\P{\frac{X_1-\bar{X}}{s_n(X)}>\frac{c-\bar{X}}{s_n(X)}\given  S}  
      \end{displaymath}
      Itt $\eta=\frac{X_1-\bar{X}}{s_n(X)}$ kísérő statisztika és $S$-től független
      $\bar{X}=\frac1n\sum X_i$, $s^2_n(X)=\frac1n\sum X_i^2-\zjel{\frac1n\sum X_i}^2$ a feltételből kiolvasható.
      \begin{displaymath}
        \E{T\given  S}=\P{\eta>\frac{c-\bar{X}}{s_n(X)}\given  S}
        =\P{\eta>x}|_{x=\frac{c-\bar{X}}{s_n(X)}}
        =1-F_\eta\zfrac*{c-\bar{X}}{s_n(X)}.
      \end{displaymath}
      \item $\eta$-ról kiszámolható, hogy 
      $\frac{1}{n-1}\eta^2\sim\text{Béta}(\frac12,\frac{n-2}2)$ és $\eta\eqinlaw-\eta$, azaz 
      $\eta$ szimmetrikus eloszlású, amiből $F_\eta$ megkapható.
    \end{itemize}
  \end{frame}

  
  \begin{frame}{Alsó korlát torzítatlan becslés szórásnégyzetére}
    \begin{itemize}
      \item $\P[0]$ $\P[1]$ eloszlások, $f_0$, $f_1$ sűrűséggel az $\mfX$ mintatéren, $\P[1]\ll\P[0]$.
      \item $T$ statisztika, $\E[i]{T}=g(i)$ $i=0,1$. 
      \begin{displaymath}
        g(1)-g(0)=\E[1]{T}-\E[0]{T}=\E[0]{T\zjel*{\frac{f_1}{f_0}-1}}
      \end{displaymath}
      \item $\E[0]{f_1/f_0-1}=0$ és $U=f_1/f_0-1$ jelöléssel
      \begin{displaymath}
        \abs{g(1)-g(0)}=\abs{\cov_0(T,U)}\leq\D[0]{T}\D[0]{U}
        \quad\implies\quad
        \D[0]^2{T}\geq \frac{(g(1)-g(0))^2}{\D[0]^2(U)}.
      \end{displaymath}
    \end{itemize}
    \continue
    Példa
    \begin{itemize}
      \item $X$ $n$ elemű minta $U(0,\theta)$ eloszlásból, $\theta>0$.
      \item $T$ torzítatlan becslés $\theta$-ra. 
      \item $0< \theta_1=\alpha\theta_0<\theta_0$, mellett $\P[1]\ll\P[0]$ és 
      $U(x)=\frac{f_1(x)}{f_0(x)}-1$, 
      $\frac{f_1(x)}{f_0(x)}%=\zfrac*{\theta_0}{\theta_1}^n\I{\max x_i<\theta_1}
      =\alpha^{-n}\I{\max x_i<\theta_1}$.
      \begin{displaymath}
        \D[0]^2{U}=\E[0]{U^2}=\E{\zfrac*{f_1}{f_0}^2}-1=\alpha^{-n}-1
        %\zjel*{\alpha^{-n}-1}^2\cdot\zfrac*{\theta_1}{\theta_0}^n
        %+1^2\cdot\zjel*{1-\zfrac*{\theta_1}{\theta_0}^n}=\alpha^{-n}-1
        \implies \D[0]^2{T}\geq 
        \theta_0^2 \sup_{\alpha\in(0,1)}\frac{(1-\alpha)^2}{\alpha^{-n}-1}
        \geq C\frac{\theta_0^2}{n^2},\quad\zjel{\alpha=1-\tfrac1n}.
        %\frac{(1-\alpha^n)^2}{\alpha^n}+1-\alpha^n(1-\alpha^n)\alpha^{-n}=\alpha^{-n}+1
      \end{displaymath}
    \end{itemize}
  \end{frame}

  \begin{frame}{Példa. Indikátor minta}
    % \begin{itemize}
    %   \item $\P[0]$ $\P[1]$ eloszlások, $f_0$, $f_1$ sűrűséggel az $\mfX$ mintatéren, $\P[1]\ll\P[0]$.
    %   \item $T$ statisztika, $\E[i]{T}=g(i)$ $i=0,1$. 
    %   \item $\E[0]{f_1/f_0-1}=0$ és $U=f_1/f_0-1$ jelöléssel
    %   \begin{displaymath}
    %     \abs{g(1)-g(0)}=\abs{\cov_0(T,U)}\leq\D[0]{T}\D[0]{U}
    %     \quad\implies\quad
    %     \D[0]^2{T}\geq \frac{(g(1)-g(0))^2}{\D[0]^2(U)}.
    %   \end{displaymath}
    % \end{itemize}
    % \continue
    % Példa
    \begin{itemize}
      \item $X$ $n$ elemű minta $\theta$ paraméterű indikátor mintából, $\theta\in(0,1)$. 
      \item $T$ torzítatlan becslés $p(\theta)$-ra, ahol $p$ legfeljebb $n$-edfokú polinom. 
      \item $\P[\theta]$ ekvivalens a számláló mértékkel minden $\theta$ mellett és 
      $\theta_1,\theta_0\in(0,1)$ esetén 
      \begin{displaymath}
        \lim_{\theta_1\to\theta_0}\frac1{\theta_1-\theta_0}\zjel*{\frac{f_{\theta_1}(x)}{f_{\theta_0}(x)}-1}
        =\partial_\theta \ln f_{\theta_0}(x)=\frac{s}{\theta_0}-\frac{n-s}{1-\theta_0},\quad\text{ahol $s=\sum x_i$}.
        % \frac{f_{\theta_1(x)}}{f_\theta_0(x)}=\frac{\theta_1^s(1-\theta_1)^{n-s}}{\theta_0^s(1-\theta_0)^{n-s}},\quad\text{ahol $s=\sum x_i$},
        % \quad
        % \E[\theta_0]{\zfrac{f_{\theta_1}}{f_\theta_0}^2}
        % =\sum_{s=0}^n \binom{n}{s}\frac{\theta_1^{2s}(1-\theta_1)^{2(n-s)}}{\theta_0^s(1-\theta_0)^{n-s}}
        % =\zjel*{\frac{\theta^2_1}{\theta_0}+\frac{(1-\theta_1)^2}{1-\theta_0}}^n=
      \end{displaymath}
      \item 
      \begin{displaymath}
        p'(\theta_0)=\lim_{\theta_1\to\theta_0}\frac{\E[\theta_0]{T\zjel*{\frac{f_{\theta_1}}{f_{\theta_0}}-1}}}{\theta_1-\theta_0}
        =\cov_{\theta_0}(T,\partial_\theta\ln f_{\theta_0})\leq \D[\theta_0]{T}\D[\theta_0]{\partial_\theta\ln f_{\theta_0}}
      \end{displaymath}
      \item
      \begin{displaymath}
        \D[\theta]{T}\geq \frac{\zjel{p'(\theta)}^2}{\D[\theta]^2{\partial_\theta\ln f_\theta}}.
      \end{displaymath}
      Ez a Cramer-Rao egyenlőtlenség speciális esete.
    \end{itemize}
  \end{frame}

  \begin{frame}{Cramer-Rao egyenlőtlenség, információs határ}
    \begin{theorem}
    \begin{itemize}[<*>]
      \item $\cP=\set{\P[\theta]}{\theta\in\Theta}$ dominált család $f_\theta$ sűrűségekkel, $\Theta\subset\real^p$. 
      \item $\ell(\theta)=\ell(\theta,x)=\ln f_\theta(x)$ a \textbf{loglikelihood} függvény. 
      \item $\theta\in\interior\Theta$, $\ell'=\ell'(\theta)=\partial_\theta\ell(\theta)$ ($p$-dimenziós sorvektor) 
      létezik és $\E[\theta]{\ell'}=0$.
      \item $I(\theta)=\Sigma_\theta((\ell'(\theta))^T)
      =\E[\theta]{\ell'(\theta))^T\ell'(\theta)}$ invertálható. $I(\theta)$ neve \textbf{Fisher információ}.
      \item $T:\mfX\to\real^q$ statisztikára $g(\theta)=\E[\theta]{T}$ deriválható és 
      $g'(\theta)=\E[\theta]{T\ell'(\theta)}$
    \end{itemize}
    Ekkor 
    \begin{displaymath}
      \Sigma_{\theta}(T)\leq G I^{-1}(\theta)G^T,\quad\text{ahol $G=g'(\theta)\in\real^{q\times p}$ és }
    \end{displaymath}
    \end{theorem}
    \begin{itemize}
      \item $A\in\real^{q\times p}$ mátrixszal $T-A\partial_\theta\ell(\theta)$ $q$-dimenziós vektor változó.
      % \item $\ell'=\partial_\theta\ell(\theta)$ jelöléssel 
      \begin{align*}
        0\leq \Sigma_\theta(T-A(\ell')^T)
        &=\Sigma_\theta(T)-A\E[\theta]{(\ell')^T T^T}-\E[\theta]{T\ell'}A^T+A\E[\theta]{(\ell')^T\ell'}A^T \\
        &=\Sigma_\theta(T)-AG^T-GA^T+A I(\theta) A^T 
      \end{align*}
      \item Ha itt $A=G^TI^{-1}(\theta)$, akkor
       \begin{displaymath}
         0\leq \Sigma_\theta(T)-G^TI^{-1}(\theta)G\quad\implies\quad \Sigma_\theta(T)\geq G^TI^{-1}(\theta) G.
       \end{displaymath}
    \end{itemize}
    %\continue
    %$I(\theta)$ neve \textbf{Fisher} információ. Az ún. \textbf{gyenge regularitási feltétel} garantálja, hogy
    %a $T$-re kirótt  minimális feltételek mellett a tétel feltételei teljesüljenek.
  \end{frame}

  \begin{frame}{Fisher információ}
    
  \end{frame}

  \begin{frame}{Gyenge regularitási feltétel}
    
  \end{frame}

  \begin{frame}{Fisher információ tulajdonságai gyenge regularitási feltétel mellett}
    
  \end{frame}
  
  \begin{frame}
  
  \end{frame}

  \end{document}

  \begin{frame}{Nem minden eloszláscsalád exponenciális}
    \begin{itemize}
      \item Ha $\cP$ exponenciális eloszláscsalád, akkor $\P\in\cP$ mértékek sűrűségek mindenhol pozitívak, 
      azaz ekvivalensek.
  
      $n$ elemű minta $U(\theta,\theta+1)$, $\theta\in\real$ eloszlásból.
      A kapott eloszláscsalád nem exponenciális.
      \item Cauchy eloszlás eltolás paraméteres családja: $X-\mu\sim \text{Cauchy}$, $\mu\in\real$.
      
      $X$ eloszlásainak a családja alkothat-e exponenciális családot?
  
      Indirekt feltevéssel %akkor $n$-elemű minta eloszlásainak a családja ismeretlen
      \begin{displaymath}
        \ln f_\mu(x)=\sum_i \ln f_\mu(x_i)=\theta(\mu)T_n(x)-n b(\theta(\mu))+\ln h(x), \quad T_n(x)=\sum_i T(x_i) 
      \end{displaymath}
      ahol $\theta,T:\real\to\real^p$. $\theta(0)=0$ feltehető, mert $\theta(0)T_n(x)$ beolvasztható $\ln h(x)$-be 
      és így a domináló mértékbe.
  
      Feltehető, hogy $\Theta=\set{\theta(\mu)}{\mu\in\real}$ %értékkészletének 
      lineáris burka $\real^p$. Ekkor létezik $\mu_k$ $k=1,\dots,p$, 
      hogy $\theta_k=\theta(\mu_k)$, $k=0,\dots,p$ lineárisan függetlenek. Ekkor alkalmas $c_k$ együtthatókkal.
      \begin{displaymath}
        T_n(x)=\sum_{k=1}^p c_k (\ln f_{\mu_k}-\ln f_0)(x).
      \end{displaymath}
      $n>p+1$ esetén $T_n(x)=T_n(y)$ akkor is előfordul, ha $x^*\neq y^*$, ami ellentmond annak, hogy a 
      rendezett minta minimális elégséges statisztika.
  
    \end{itemize}
    
  \end{frame}
  
  \end{document}
  
  \begin{frame}{Momentum generáló függvény, deriválhatóság}
    \begin{proposition}
      Ha $M (\alpha)=\E{e^{\alpha\cdot X}}<\infty$, az origó egy környezetében,
      akkor ott  $M$ deriválható.
    \end{proposition}
    \begin{itemize}
      
    \item  Létezik $\eps>0$, hogy $\E{e^{h\norm{X}}}<\infty$, ha
      $h<\eps$.
  
      $\real^p$ a normák ekvivalensek, elég
      $\norm{x}_{1}=\frac1p \sum \abs*{x_i}$-re számolni. Mivel $\exp$
      konvex $e^{\norm{x}_1}\leq\frac1p \sum e^{\abs*{x_i}}$ és
      %$\norm{h}_1=\sum h_i=1$ és $h_i\geq0$, akkor
      \begin{displaymath}
        e^{h\abs*{X_i}}\leq e^{hX_i}+e^{-hX_i},\quad
        \E{e^{h\norm{X}_1}}\leq \frac1p \sum_i(M (h e_i)+M (-h e_i))<\infty
      \end{displaymath}
      ahol $e_1,\dots,e_p$ a természetes bázis $\real^p$-ben.
    \item ha $\norm{\alpha}$ kicsi, akkor
      $\abs**{\alpha\cdot X}<\eps\norm{X}$ és a dominált konvergencia tétel miatt
      \begin{displaymath}
        M (\alpha)=\E{\sum_k \frac{(\alpha\cdot X)^n}{n!}}
        =\sum_n \frac{\E{(\alpha\cdot X)^n}}{n!}\quad\implies\quad 
        \partial_{i_1,\dots,i_k}M (0) =\E{\prod_{j}X_{i_j}}
      \end{displaymath}
    \end{itemize}
  
  \end{frame}
  
  \end{document}
  